<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>NeoChat • WebLLM (Browser‑Only)</title>
  <meta name="description" content="A futuristic, lightweight in‑browser LLM chat that runs entirely client‑side via WebGPU. Host on GitHub Pages for free." />
  <!-- Tailwind (CDN build) -->
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          fontFamily: {
            display: ['Inter', 'ui-sans-serif', 'system-ui', 'Segoe UI', 'Roboto', 'Helvetica Neue', 'Arial'],
          },
          boxShadow: {
            glow: '0 0 40px rgba(99,102,241,.35)',
          },
          keyframes: {
            gradient: {
              '0%, 100%': { backgroundPosition: '0% 50%' },
              '50%': { backgroundPosition: '100% 50%' },
            },
          },
          animation: {
            gradient: 'gradient 12s ease infinite',
          },
        }
      }
    }
  </script>
  <style>
    /* Futuristic vibe */
    .glass { backdrop-filter: blur(14px); background: rgba(255,255,255,0.06); border: 1px solid rgba(255,255,255,0.12); }
    .scrollbar::-webkit-scrollbar { width: 10px; }
    .scrollbar::-webkit-scrollbar-thumb { background: rgba(255,255,255,.18); border-radius: 9999px; }
    .token { opacity:.9 }
  </style>
</head>
<body class="min-h-screen text-slate-100 bg-slate-950 relative">
  <!-- Animated gradient halo -->
  <div class="pointer-events-none absolute inset-0 -z-10 bg-[radial-gradient(65%_45%_at_50%_35%,rgba(99,102,241,.35),transparent_60%),radial-gradient(45%_35%_at_80%_20%,rgba(16,185,129,.25),transparent_60%),radial-gradient(45%_35%_at_20%_80%,rgba(59,130,246,.25),transparent_60%)]"></div>
  <div class="absolute inset-0 -z-20 bg-gradient-to-br from-slate-950 via-indigo-950/30 to-slate-900 animate-gradient bg-[length:200%_200%]"></div>

  <!-- Header -->
  <header class="max-w-6xl mx-auto px-4 pt-8 pb-4 flex items-center justify-between">
    <div class="flex items-center gap-3">
      <div class="h-10 w-10 rounded-2xl bg-gradient-to-br from-indigo-500 via-fuchsia-500 to-emerald-400 shadow-glow"></div>
      <div>
        <h1 class="text-xl md:text-2xl font-semibold tracking-tight">NeoChat <span class="text-indigo-400">• WebLLM</span></h1>
        <p class="text-xs md:text-sm text-slate-400">100% in‑browser LLM • No server • Free on GitHub Pages</p>
      </div>
    </div>
    <a href="#" id="btnGPU" class="text-xs px-3 py-1.5 rounded-full glass hover:bg-white/10 transition">Checking WebGPU…</a>
  </header>

  <!-- Main card -->
  <main class="max-w-6xl mx-auto px-4 pb-10">
    <div class="grid md:grid-cols-[320px,1fr] gap-4">
      <!-- Left: Controls -->
      <aside class="glass rounded-3xl p-4 md:p-5">
        <h2 class="text-sm font-semibold uppercase tracking-wider text-slate-300 mb-3">Model</h2>
        <div class="space-y-3">
          <label class="block text-sm">Choose a tiny, browser‑friendly model</label>
          <select id="modelSelect" class="w-full glass rounded-xl px-3 py-2 bg-white/5 text-sm outline-none focus:ring-2 focus:ring-indigo-400">
            <option value="Qwen2.5-0.5B-Instruct-q4f32_1-MLC">Qwen2.5‑0.5B‑Instruct (q4f32_1)</option>
            <option value="Qwen2.5-1.5B-Instruct-q4f32_1-MLC">Qwen2.5‑1.5B‑Instruct (q4f32_1)</option>
            <option value="Llama-3.2-1B-Instruct-q4f32_1-MLC">Llama‑3.2‑1B‑Instruct (q4f32_1)</option>
            <option value="SmolLM2-1.7B-Instruct-q4f16_1-MLC">SmolLM2‑1.7B‑Instruct (q4f16_1)</option>
          </select>

          <button id="btnLoad" class="w-full mt-1 inline-flex items-center justify-center gap-2 rounded-xl bg-indigo-500/90 hover:bg-indigo-400 px-3 py-2.5 text-sm font-semibold shadow-glow transition">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M4 7h16M4 12h16M4 17h16" /></svg>
            Load / Switch Model
          </button>

          <div id="progressWrap" class="hidden rounded-xl border border-white/10 bg-black/20 p-3 text-xs">
            <div class="flex items-center justify-between mb-1">
              <span class="text-slate-300">Download & init</span>
              <span id="progressPct" class="text-slate-400">0%</span>
            </div>
            <div class="w-full h-2 rounded-full bg-white/10 overflow-hidden">
              <div id="progressBar" class="h-2 bg-indigo-400 rounded-full w-0 transition-all"></div>
            </div>
            <div id="progressNote" class="mt-2 text-slate-400"></div>
          </div>
        </div>

        <h2 class="text-sm font-semibold uppercase tracking-wider text-slate-300 mt-6 mb-3">System Prompt</h2>
        <textarea id="systemPrompt" class="w-full h-24 glass rounded-xl px-3 py-2 bg-white/5 text-sm outline-none focus:ring-2 focus:ring-indigo-400" placeholder="You are NeoChat, a concise multilingual assistant (MS + EN). Be helpful, safe, and fast."></textarea>

        <div class="mt-4 flex items-center justify-between">
          <label class="text-sm flex items-center gap-2"><input id="chkStream" type="checkbox" class="accent-indigo-400" checked /> Streaming</label>
          <button id="btnClear" class="text-sm px-3 py-1.5 rounded-lg glass hover:bg-white/10">Clear Chat</button>
        </div>
      </aside>

      <!-- Right: Chat -->
      <section class="glass rounded-3xl overflow-hidden flex flex-col min-h-[70vh]">
        <!-- Chat header -->
        <div class="px-4 md:px-6 py-3 flex items-center justify-between border-b border-white/10">
          <div class="text-sm text-slate-300">Status: <span id="status">Idle</span></div>
          <div class="text-xs text-slate-400">Runs locally in your browser • WebGPU</div>
        </div>

        <!-- Messages -->
        <div id="chat" class="flex-1 overflow-y-auto p-4 md:p-6 space-y-4 scrollbar">
          <div class="text-sm text-slate-400">Load a model to begin. First load downloads model weights and may take a minute; future loads are cached.</div>
        </div>

        <!-- Composer -->
        <form id="composer" class="border-t border-white/10 p-3 md:p-4 flex items-end gap-2">
          <textarea id="input" rows="1" placeholder="Type your message…" class="flex-1 resize-none rounded-2xl bg-white/5 px-4 py-3 outline-none focus:ring-2 focus:ring-indigo-400"></textarea>
          <button class="rounded-2xl bg-indigo-500/90 hover:bg-indigo-400 px-5 py-3 font-semibold shadow-glow">Send</button>
        </form>
      </section>
    </div>

    <!-- Footer -->
    <div class="text-center text-xs text-slate-500 mt-8">
      Built with <a class="underline decoration-indigo-400/40 hover:text-slate-300" href="https://webllm.mlc.ai/" target="_blank" rel="noreferrer">WebLLM</a>. Host this single file on GitHub Pages.
    </div>
  </main>

  <!-- App: ESM import of WebLLM from jsDelivr -->
  <script type="module">
    import * as webllm from 'https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.79/lib/index.js';

    // DOM helpers
    const $ = sel => document.querySelector(sel);
    const chatEl = $('#chat');
    const inputEl = $('#input');
    const composerEl = $('#composer');
    const statusEl = $('#status');
    const modelSelect = $('#modelSelect');
    const btnLoad = $('#btnLoad');
    const btnClear = $('#btnClear');
    const btnGPU = $('#btnGPU');
    const progressWrap = $('#progressWrap');
    const progressPct = $('#progressPct');
    const progressBar = $('#progressBar');
    const progressNote = $('#progressNote');
    const chkStream = $('#chkStream');
    const systemPromptEl = $('#systemPrompt');

    // WebGPU availability check
    const hasWebGPU = !!navigator.gpu;
    btnGPU.textContent = hasWebGPU ? 'WebGPU ✓' : 'WebGPU ✗ (Enable Chrome flags or update)';
    if (!hasWebGPU) btnGPU.classList.add('bg-red-500/20');

    // Engine instance (created after first load)
    let engine = null;
    let history = [];

    function addBubble(role, html) {
      const wrap = document.createElement('div');
      wrap.className = `flex ${role==='user' ? 'justify-end' : 'justify-start'}`;
      const bubble = document.createElement('div');
      bubble.className = `${role==='user' ? 'bg-indigo-500/90' : 'bg-white/5'} max-w-[85%] rounded-2xl px-4 py-3 text-sm leading-relaxed whitespace-pre-wrap`;
      bubble.innerHTML = html;
      wrap.appendChild(bubble);
      chatEl.appendChild(wrap);
      chatEl.scrollTop = chatEl.scrollHeight;
      return bubble;
    }

    function setStatus(s) { statusEl.textContent = s; }

    function showProgress(show, note='') {
      progressWrap.classList.toggle('hidden', !show);
      if (note) progressNote.textContent = note;
    }

    const initProgressCallback = (p) => {
      // p = { progress: 0..1, text: string }
      const pct = Math.round((p?.progress || 0) * 100);
      progressPct.textContent = pct + '%';
      progressBar.style.width = pct + '%';
      progressNote.textContent = p?.text || '';
    };

    async function loadModel() {
      try {
        const selected = modelSelect.value;
        setStatus('Loading model…');
        showProgress(true, 'Downloading weights & compiling…');

        if (!engine) {
          // Recommended init API
          engine = await webllm.CreateMLCEngine(selected, { initProgressCallback });
        } else {
          await engine.reload(selected);
        }

        showProgress(false);
        setStatus(`Ready · ${selected}`);

        // Seed system prompt into history (optional)
        history = [];
        const sys = systemPromptEl.value.trim();
        if (sys) history.push({ role: 'system', content: sys });

        addBubble('assistant', `✅ <b>${selected}</b> loaded. Ask me anything.`);
      } catch (err) {
        console.error('[WebLLM] Load error:', err);
        setStatus('Error loading');
        showProgress(false);
        addBubble('assistant', `❌ Failed to load model. Open the browser console (F12 → Console) and share the red error message.`);
      }
    }

    btnLoad.addEventListener('click', loadModel);
    btnClear.addEventListener('click', () => { chatEl.innerHTML=''; history = []; setStatus('Idle'); });

    composerEl.addEventListener('submit', async (e) => {
      e.preventDefault();
      const text = inputEl.value.trim();
      if (!text) return;
      addBubble('user', text);
      inputEl.value = '';
      if (!engine) { addBubble('assistant', 'ℹ️ Load a model first.'); return; }

      const userMsg = { role: 'user', content: text };
      const msgs = [...history, userMsg];
      setStatus('Thinking…');

      const stream = chkStream.checked;
      if (stream) {
        // Stream tokens as they arrive
        const chunks = await engine.chat.completions.create({ messages: msgs, stream: true });
        const holder = addBubble('assistant', '<span class="token"></span>');
        const tokenSpan = holder.querySelector('.token');
        let acc = '';
        for await (const chunk of chunks) {
          const delta = chunk?.choices?.[0]?.delta?.content || '';
          acc += delta;
          tokenSpan.textContent = acc;
          chatEl.scrollTop = chatEl.scrollHeight;
        }
        history.push(userMsg, { role: 'assistant', content: acc });
      } else {
        // Single response
        const res = await engine.chat.completions.create({ messages: msgs });
        const msg = res?.choices?.[0]?.message?.content || '(no content)';
        addBubble('assistant', msg);
        history.push(userMsg, { role: 'assistant', content: msg });
      }

      setStatus('Ready');
      inputEl.focus();
    });

    // Prefill a sensible default system prompt
    systemPromptEl.value = 'You are NeoChat, a concise bilingual assistant (Malay + English). Keep answers short, helpful, and safe. If code is requested, provide runnable snippets.';
  </script>
</body>
</html>
