<div align="center">

<p align="center">
  <img src="./banner.svg" alt="NeoChat • WebLLM banner" width="100%"/>
</p>


# 🧠 **NeoChat • WebLLM**  
### _A Browser-Native AI Built for the Future_  

🌐 **Live Demo:** [msharizal.github.io/neochat-webllm](https://msharizal.github.io/neochat-webllm)  
🧩 Powered by [WebLLM (MLC.ai)](https://webllm.mlc.ai) • ⚙️ Runs Locally • 🔐 100% Private  

</div>

---

### ✨ Overview  
**NeoChat** is a **next-generation, browser-based LLM** that runs **entirely on your device** — no servers, no API keys, and zero data leakage.  
Powered by **WebGPU**, it demonstrates the future of **client-side AI**: lightweight, private, and blazing fast.  
The entire app lives in one elegant file: `index.html`.

---

### ⚡ Features  
- 🧩 **100% Local Execution** – inference runs directly in your browser with WebGPU  
- 🌍 **Bilingual Support** – optimized for English + Malay (Qwen-Instruct models)  
- 🧠 **Model Switcher** – toggle between Qwen 2.5, Llama 3.2 & SmolLM  
- 💫 **Glassmorphic UI** – minimalist, futuristic interface powered by TailwindCSS  
- 💬 **Streaming Generation** – token-by-token responses in real time  
- 🔐 **Privacy-First** – nothing leaves your device, ever  
- 🪶 **One-File Architecture** – deploy anywhere instantly (e.g., GitHub Pages)

---

### 🧰 Tech Stack  

| Component | Description |
|------------|-------------|
| **WebLLM** | WebGPU-accelerated LLM inference |
| **TailwindCSS** | Utility-first design system |
| **HTML + ES Modules** | Lightweight, framework-free front-end |
| **GitHub Pages** | Free static hosting for AI web apps |

---

### ⚙️ Quick Setup (Local Use)  

> 🧠 **Note:** WebGPU support is required. Follow these steps to enable it.

**1️⃣ Update Browser**  
- **Chrome:** ⋮ → Help → About Google Chrome  
- **Edge:** … → Help and feedback → About Microsoft Edge  

---

#### 2️⃣ **Enable WebGPU Flag**
Paste one of these in your address bar and set to **Enabled → Relaunch**

- 🔗 **[Open in Chrome](chrome://flags/#enable-unsafe-webgpu)**  
- 🔗 **[Open in Edge](edge://flags/#enable-unsafe-webgpu)**  

---

**3️⃣ Enable Hardware Acceleration**  
Settings → System → “Use hardware acceleration when available” → ON → Restart browser  

**4️⃣ Force GPU Mode (Windows)**  
System → Display → Graphics → Select Chrome → Options → High performance → Save  

---

### 🧠 Recommended Models  

| Model | Size | Notes |
|--------|------|-------|
| `Qwen2.5-0.5B-Instruct-q4f32_1-MLC` | 🪶 Tiny | Fast & multilingual |
| `Qwen2.5-1.5B-Instruct-q4f32_1-MLC` | ⚖️ Medium | Balanced performance |
| `Llama-3.2-1B-Instruct-q4f32_1-MLC` | 💡 Medium | Great English fluency |
| `SmolLM2-1.7B-Instruct-q4f16_1-MLC` | 🧩 Large | Better reasoning depth |

_First download may take a minute; future sessions load instantly from cache._

---

### 🪩 Vision  
> “The future of AI isn’t on distant servers — it’s right in your browser.”  

**NeoChat** is a glimpse into the **WebGPU era**, where AI inference is local, secure, and lightning fast.  
Build. Fork. Remix. Deploy your own **personal LLM portal** — no cloud required.  

---

### 👨‍🚀 Author  
**Sharizal Shafiq**  
[🔗 LinkedIn](https://www.linkedin.com/in/msharizal) • [💻 GitHub](https://github.com/MSharizal)

---

### 🪪 License  
**MIT License © 2025** – Free to use, modify, and deploy.

---

<div align="center">

⚡ **Built for the WebGPU Generation**  
💜 **NeoChat — Where AI Meets the Browser**

</div>
